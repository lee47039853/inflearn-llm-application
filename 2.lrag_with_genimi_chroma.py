#!/usr/bin/env python3
"""
RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ ì˜ˆì œ
ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
"""

from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain import hub
from langchain.chains import RetrievalQA
from pathlib import Path
import shutil
import logging

def check_existing_database():
    """ê¸°ì¡´ Chroma ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    chroma_dir = Path("./chroma")
    if chroma_dir.exists() and any(chroma_dir.iterdir()):
        return True
    return False

def clear_database():
    """ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ"""
    chroma_dir = Path("./chroma")
    if chroma_dir.exists():
        shutil.rmtree(chroma_dir)
        print("ğŸ—‘ï¸  ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

def get_user_choice():
    """ì‚¬ìš©ìë¡œë¶€í„° ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ë°©í–¥ ì„ íƒë°›ê¸°"""
    print("\nğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ì˜µì…˜:")
    print("1. ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì¬ì‚¬ìš© (ë¹ ë¦„)")
    print("2. ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ì²˜ìŒ ì‹¤í–‰)")
    print("3. ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
            if choice in ['1', '2', '3']:
                return choice
            else:
                print("âŒ 1, 2, 3 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit()

def get_debug_mode():
    """ë””ë²„ê·¸ ëª¨ë“œ ì„ íƒ"""
    print("\nğŸ” ë””ë²„ê·¸ ì˜µì…˜:")
    print("1. ê¸°ë³¸ ëª¨ë“œ (í”„ë¡¬í”„íŠ¸ ë¯¸í‘œì‹œ)")
    print("2. í”„ë¡¬í”„íŠ¸ í™•ì¸ ëª¨ë“œ (LLM ì „ë‹¬ í”„ë¡¬í”„íŠ¸ í‘œì‹œ)")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
            if choice in ['1', '2']:
                return choice == '2'
            else:
                print("âŒ 1, 2 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit()



def main():
    # LangChain ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™” (ì„ íƒì‚¬í•­)
    # logging.basicConfig(level=logging.DEBUG)
    
    # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # Google API í‚¤ í™•ì¸
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— GOOGLE_API_KEY=your_api_key_hereë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    try:
        # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
        has_existing_db = check_existing_database()
        
        if has_existing_db:
            choice = get_user_choice()
            
            if choice == '3':
                clear_database()
                has_existing_db = False
        else:
            print("ğŸ“„ ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ë””ë²„ê·¸ ëª¨ë“œ ì„ íƒ
        debug_mode = get_debug_mode()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print("\nğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ì‚¬ìš©ìë¡œë¶€í„° ì„ë² ë”© ëª¨ë¸ ì„ íƒë°›ê¸°
        print("\nğŸ“Š ì„ë² ë”© ëª¨ë¸ ì„ íƒ:")
        print("1. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (ko-sroberta-multitask) - ì¶”ì²œ")
        print("2. Google Gemini ì„ë² ë”© (ê¸°ì¡´)")
        
        while True:
            try:
                embedding_choice = input("\nì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
                if embedding_choice in ['1', '2']:
                    break
                else:
                    print("âŒ 1, 2 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                exit()
        
        if embedding_choice == '1':
            # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            model_name = "jhgan/ko-sroberta-multitask"
            model_kwargs = {'device': 'cpu'}
            encode_kwargs = {'normalize_embeddings': True}
            embedding = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )
            print("âœ… í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        else:
            # Google Gemini ì„ë² ë”© ì‚¬ìš© (ê¸°ì¡´)
            print("ğŸŒ Google Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            print("âœ… Google Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ") 

        # Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        database = Chroma(
            collection_name='chroma-tax', 
            persist_directory="./chroma", 
            embedding_function=embedding
        )

        # ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬
        if has_existing_db and choice == '1':
            print("ğŸ“š ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì‚¬ìš© ì¤‘...")
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸
            try:
                collection_count = database._collection.count()
                print(f"âœ… ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {collection_count}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âš ï¸  ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                print("   ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                has_existing_db = False
        
        if not has_existing_db or choice == '2':
            print("ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1500,
                chunk_overlap=200,
            )

            # Word ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
            loader = Docx2txtLoader("./tax.docx")
            document_list = loader.load_and_split(text_splitter=text_splitter)
            
            print(f"âœ… {len(document_list)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

            # ë¬¸ì„œë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            database.add_documents(document_list)
            print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash', temperature=0.9)
        prompt = hub.pull("rlm/rag-prompt")

        # RAG ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm, 
            retriever=database.as_retriever(),
            chain_type_kwargs={"prompt": prompt}
        )
        


        # ì§ˆë¬¸-ë‹µë³€ ë£¨í”„
        while True:
            # ì‚¬ìš©ìë¡œë¶€í„° ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
            print("\nğŸ¤– ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
            print("   (ì˜ˆ: ì œ 55ì¡°ì˜ ì¢…í•©ì†Œë“ ê³¼ì œí‘œì¤€ ê¸°ì¤€ìœ¼ë¡œ ê±°ì£¼ìì˜ ì—°ë´‰ì´ 5ì²œë§Œì› ì¸ ê²½ìš° í•´ë‹¹ ê±°ì£¼ìì˜ ì†Œë“ì„¸ëŠ” ì–¼ë§ˆì¸ê°€ìš”?)")
            print("   (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥)")
            print("-" * 50)
            
            try:
                query = input("ì§ˆë¬¸: ").strip()
                if not query:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    return
                
                print(f"\nğŸ¤– ì…ë ¥ëœ ì§ˆë¬¸: {query}")
                print("-" * 50)
                
                # ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
                retrieved_docs_with_scores = database.similarity_search_with_score(query)
                retrieved_docs = [doc for doc, _ in retrieved_docs_with_scores]
                print(f"ğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ë¶„ì„
                print("\nğŸ” ìœ ì‚¬ë„ ì ìˆ˜ ë¶„ì„:")
                print("=" * 60)
                for i, (doc, score) in enumerate(retrieved_docs_with_scores, 1):
                    print(f"\nğŸ“‹ ë¬¸ì„œ {i} (ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}):")
                    print(f"   í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}")
                    print(f"   ì†ŒìŠ¤: {doc.metadata.get('source', 'N/A')}")
                    print(f"   ë‚´ìš© ê¸¸ì´: {len(doc.page_content)}ì")
                    print(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:200]}...")
                    print("-" * 40)
                print("=" * 60)
                
                # ìœ ì‚¬ë„ ì ìˆ˜ í•´ì„
                print("\nğŸ’¡ ìœ ì‚¬ë„ ì ìˆ˜ í•´ì„:")
                print("   - ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ë” ìœ ì‚¬í•¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)")
                print("   - ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ëœ ìœ ì‚¬í•¨")
                
                # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
                best_match = min(retrieved_docs_with_scores, key=lambda x: x[1])
                print(f"\nğŸ† ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: ë¬¸ì„œ {retrieved_docs_with_scores.index(best_match) + 1} (ì ìˆ˜: {best_match[1]:.4f})")
                
                # ì „ì²´ ë¬¸ì„œ ë‚´ìš© ì¶œë ¥
                print("\nğŸ“„ ì „ì²´ ë¬¸ì„œ ë‚´ìš©:")
                print("=" * 60)
                for i, (doc, score) in enumerate(retrieved_docs_with_scores, 1):
                    print(f"\nğŸ“‹ ë¬¸ì„œ {i} (ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}):")
                    print(f"   ë‚´ìš©: {doc.page_content}")
                    print("-" * 40)
                print("=" * 60)

                # ë””ë²„ê·¸ ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í™•ì¸
                if debug_mode:
                    print("\nğŸ” LLMì— ì „ë‹¬ë˜ëŠ” í”„ë¡¬í”„íŠ¸ í™•ì¸:")
                    print("=" * 60)
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ contextë¡œ ê²°í•©
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° í‘œì‹œ
                    print("ğŸ“‹ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:")
                    print("Context: [ê²€ìƒ‰ëœ ë¬¸ì„œë“¤]")
                    print("Question: [ì‚¬ìš©ì ì§ˆë¬¸]")
                    print("Answer: [AI ì‘ë‹µ]")
                    print("-" * 40)
                    
                    # ì‹¤ì œ ì „ë‹¬ë˜ëŠ” ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                    print("ğŸ“ ì‹¤ì œ ì „ë‹¬ë˜ëŠ” ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
                    print(f"ì§ˆë¬¸: {query}")
                    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
                    print(f"Context ê¸¸ì´: {len(context)}ì")
                    print(f"Context ë¯¸ë¦¬ë³´ê¸°: {context[:300]}...")
                    print("=" * 60)
                    
                    # í”„ë¡¬í”„íŠ¸ ì •ë³´
                    print(f"ğŸ“Š í”„ë¡¬í”„íŠ¸ ì •ë³´:")
                    print(f"   - ì§ˆë¬¸ ê¸¸ì´: {len(query)}ì")
                    print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
                    print(f"   - Context ê¸¸ì´: {len(context)}ì")
                    print(f"   - ì´ ì „ë‹¬ ë°ì´í„°: {len(query) + len(context)}ì")
                    
                    # ìƒì„¸ ë°ì´í„° ë¶„ì„
                    print(f"\nğŸ“ˆ ìƒì„¸ ë°ì´í„° ë¶„ì„:")
                    print(f"   - ê° ë¬¸ì„œë³„ ê¸¸ì´:")
                    for i, doc in enumerate(retrieved_docs, 1):
                        doc_length = len(doc.page_content)
                        print(f"     ë¬¸ì„œ {i}: {doc_length}ì")
                    
                    # í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì  ê³„ì‚°)
                    estimated_tokens = (len(query) + len(context)) // 4  # ëŒ€ëµ 1í† í° = 4ì
                    print(f"   - ì¶”ì • í† í° ìˆ˜: ì•½ {estimated_tokens:,} í† í°")
                    
                    # ë°ì´í„° íš¨ìœ¨ì„± ë¶„ì„
                    avg_doc_length = len(context) / len(retrieved_docs) if retrieved_docs else 0
                    print(f"   - í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_doc_length:.0f}ì")
                    print(f"   - ë°ì´í„° íš¨ìœ¨ì„±: {len(query)}ì ì§ˆë¬¸ â†’ {len(context)}ì ì»¨í…ìŠ¤íŠ¸")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (UTF-8 ê¸°ì¤€)
                    memory_usage = (len(query) + len(context)) * 4  # UTF-8ì€ ìµœëŒ€ 4ë°”ì´íŠ¸
                    print(f"   - ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ {memory_usage:,} ë°”ì´íŠ¸ ({memory_usage/1024:.1f} KB)")
                    
                    # API ë¹„ìš© ì¶”ì • (Gemini ê¸°ì¤€)
                    input_tokens = estimated_tokens
                    output_tokens_estimate = 100  # ì˜ˆìƒ ì¶œë ¥ í† í° ìˆ˜
                    # Gemini 2.0 Flash ìš”ê¸ˆ (ì˜ˆì‹œ - ì‹¤ì œ ìš”ê¸ˆì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                    input_cost = (input_tokens / 1000) * 0.000075  # $0.000075 per 1K input tokens
                    output_cost = (output_tokens_estimate / 1000) * 0.0003   # $0.0003 per 1K output tokens
                    total_cost = input_cost + output_cost
                    print(f"   - ì¶”ì • API ë¹„ìš©: ì•½ ${total_cost:.6f} (ì…ë ¥: ${input_cost:.6f}, ì¶œë ¥: ${output_cost:.6f})")
                    
                    # ë°ì´í„° ì „ì†¡ ì‹œê°í™”
                    print(f"\nğŸ“Š ë°ì´í„° ì „ì†¡ ì‹œê°í™”:")
                    total_chars = len(query) + len(context)
                    query_percent = (len(query) / total_chars) * 100
                    context_percent = (len(context) / total_chars) * 100
                    
                    print(f"   ì§ˆë¬¸: {'â–ˆ' * int(query_percent/2)}{' ' * (50 - int(query_percent/2))} {query_percent:.1f}%")
                    print(f"   ì»¨í…ìŠ¤íŠ¸: {'â–ˆ' * int(context_percent/2)}{' ' * (50 - int(context_percent/2))} {context_percent:.1f}%")
                    print(f"   {'â”€' * 50}")
                    print(f"   ì´ {total_chars:,}ì ({estimated_tokens:,} í† í°)")
                    
                    # ì„±ëŠ¥ ìµœì í™” ì œì•ˆ
                    print(f"\nğŸ’¡ ì„±ëŠ¥ ìµœì í™” ì œì•ˆ:")
                    if len(context) > 8000:  # 8K í† í° ì´ìƒ
                        print(f"   - âš ï¸  ì»¨í…ìŠ¤íŠ¸ê°€ í½ë‹ˆë‹¤. ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ ì²­í¬ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
                    if len(retrieved_docs) > 5:
                        print(f"   - ğŸ’¡ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë§ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
                    if avg_doc_length > 1000:
                        print(f"   - ğŸ’¡ ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    print("-" * 40)
                
                # ì‹¤ì œ ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
                print("ğŸ§  AI ì‘ë‹µ ìƒì„± ì¤‘...")
                ai_message = qa_chain({"query": query})

                print("\n" + "=" * 50)
                print("âœ… ìµœì¢… ì‘ë‹µ:")
                print(ai_message['result'])
                print("=" * 50)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            except Exception as e:
                print(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("   ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                continue
        
    except FileNotFoundError:
        print("âŒ tax.docx íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— tax.docx íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()