#!/usr/bin/env python3
"""
RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ ì˜ˆì œ
ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ - ìœ ì‚¬ì–´ Dictionary ë° ì¿¼ë¦¬ ìµœì í™” íŒŒì´í”„ë¼ì¸ í¬í•¨
"""

from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
import os
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain import hub
from langchain.chains import RetrievalQA
from pathlib import Path
import shutil
import logging
import json
import re
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from datetime import datetime

class SynonymDictionary:
    """ìœ ì‚¬ì–´ Dictionary ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, synonym_file: str = "synonyms.json"):
        self.synonym_file = synonym_file
        self.synonyms = self._load_synonyms()
        self.expansion_mode = "normalize"  # ê¸°ë³¸ê°’: ì •ê·œí™” ëª¨ë“œ
    
    def _load_synonyms(self) -> Dict[str, List[str]]:
        """ìœ ì‚¬ì–´ íŒŒì¼ ë¡œë“œ"""
        default_synonyms = {
            "ì†Œë“ì„¸": ["ì†Œë“ì„¸", "ì†Œë“ì„¸ì•¡", "ì†Œë“ì„¸ê¸ˆ", "ì†Œë“ì„¸ìœ¨", "ì†Œë“ì„¸ ê³„ì‚°"],
            "ì—°ë´‰": ["ì—°ë´‰", "ì—°ê°„ì†Œë“", "ì—°ì†Œë“", "ì—°ê°„ìˆ˜ì…", "ì—°ìˆ˜ì…", "ì—°ê°„ê¸‰ì—¬"],
            "ê±°ì£¼ì": ["ê±°ì£¼ì", "ì§ì¥ì¸", "ì‚¬ëŒ","ê±°ì£¼ì„¸", "ê±°ì£¼ë¯¼", "ê±°ì£¼ìì„¸", "ê±°ì£¼ìì„¸ìœ¨"],
            "ê³¼ì„¸í‘œì¤€": ["ê³¼ì„¸í‘œì¤€", "ê³¼ì„¸ê¸°ì¤€", "ê³¼ì„¸í‘œì¤€ì•¡", "ê³¼ì„¸ê¸°ì¤€ì•¡", "ê³¼ì„¸í‘œì¤€ê¸ˆì•¡"],
            "ì¢…í•©ì†Œë“": ["ì¢…í•©ì†Œë“", "ì¢…í•©ì†Œë“ì„¸", "ì¢…í•©ì†Œë“ì„¸ìœ¨", "ì¢…í•©ì†Œë“ì„¸ì•¡"],
            "ì„¸ìœ¨": ["ì„¸ìœ¨", "ì„¸ê¸ˆìœ¨", "ì„¸ìœ¨í‘œ", "ì„¸ìœ¨ê¸°ì¤€", "ì„¸ìœ¨ê³„ì‚°"],
            "ê³µì œ": ["ê³µì œ", "ì„¸ì•¡ê³µì œ", "ì†Œë“ê³µì œ", "ê³µì œì•¡", "ê³µì œê¸ˆì•¡"],
            "ì‹ ê³ ": ["ì‹ ê³ ", "ì„¸ê¸ˆì‹ ê³ ", "ì†Œë“ì„¸ì‹ ê³ ", "ì‹ ê³ ì„œ", "ì‹ ê³ ê¸°ê°„"],
            "ë‚©ë¶€": ["ë‚©ë¶€", "ì„¸ê¸ˆë‚©ë¶€", "ë‚©ë¶€ì•¡", "ë‚©ë¶€ê¸°ê°„", "ë‚©ë¶€ë°©ë²•"],
            "ê³„ì‚°": ["ê³„ì‚°", "ì„¸ê¸ˆê³„ì‚°", "ì†Œë“ì„¸ê³„ì‚°", "ê³„ì‚°ë°©ë²•", "ê³„ì‚°ê¸°"],
            "ë²•ì¸ì„¸": ["ë²•ì¸ì„¸", "ë²•ì¸ì„¸ìœ¨", "ë²•ì¸ì„¸ì•¡", "ë²•ì¸ì„¸ìœ¨í‘œ"],
            "ë¶€ê°€ê°€ì¹˜ì„¸": ["ë¶€ê°€ê°€ì¹˜ì„¸", "ë¶€ê°€ì„¸", "ë¶€ê°€ê°€ì¹˜ì„¸ìœ¨", "ë¶€ê°€ì„¸ìœ¨"],
            "ì–‘ë„ì†Œë“ì„¸": ["ì–‘ë„ì†Œë“ì„¸", "ì–‘ë„ì„¸", "ì–‘ë„ì†Œë“ì„¸ìœ¨", "ì–‘ë„ì„¸ìœ¨"],
            "ìƒì†ì„¸": ["ìƒì†ì„¸", "ìƒì†ì„¸ìœ¨", "ìƒì†ì„¸ì•¡", "ìƒì†ì„¸ìœ¨í‘œ"],
            "ì¦ì—¬ì„¸": ["ì¦ì—¬ì„¸", "ì¦ì—¬ì„¸ìœ¨", "ì¦ì—¬ì„¸ì•¡", "ì¦ì—¬ì„¸ìœ¨í‘œ"]
        }
        
        try:
            if os.path.exists(self.synonym_file):
                with open(self.synonym_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                # ê¸°ë³¸ ìœ ì‚¬ì–´ íŒŒì¼ ìƒì„±
                with open(self.synonym_file, 'w', encoding='utf-8') as f:
                    json.dump(default_synonyms, f, ensure_ascii=False, indent=2)
                return default_synonyms
        except Exception as e:
            print(f"âš ï¸  ìœ ì‚¬ì–´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return default_synonyms
    
    def expand_query(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ë¥¼ ìœ ì‚¬ì–´ë¡œ í™•ì¥ (ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥´ê²Œ ë™ì‘)"""
        expanded_queries = [query]
        expansion_log = []  # í™•ì¥ ê³¼ì • ë¡œê·¸
        
        if self.expansion_mode == "normalize":
            # ì •ê·œí™” ëª¨ë“œ: ìœ ì‚¬ì–´ë¥¼ ëŒ€í‘œì–´ë¡œë§Œ ì •ê·œí™”
            normalized_query = self.normalize_synonyms_to_main_terms(query)
            if normalized_query != query:
                expanded_queries.append(normalized_query)
                expansion_log.append({
                    'type': 'normalization',
                    'original_query': query,
                    'normalized_query': normalized_query,
                    'description': 'ìœ ì‚¬ì–´ë¥¼ ëŒ€í‘œì–´ë¡œ ì •ê·œí™”'
                })
        
        elif self.expansion_mode == "expand":
            # í™•ì¥ ëª¨ë“œ: ëŒ€í‘œì–´ë¥¼ ìœ ì‚¬ì–´ë¡œë§Œ í™•ì¥
            # ë¨¼ì € ì •ê·œí™”
            normalized_query = self.normalize_synonyms_to_main_terms(query)
            working_query = normalized_query if normalized_query != query else query
            
            # ë‹¨ì¼ ìš©ì–´ í™•ì¥
            for main_term, synonyms in self.synonyms.items():
                if main_term in working_query:
                    for synonym in synonyms:
                        if synonym != main_term:
                            expanded_query = working_query.replace(main_term, synonym)
                            if expanded_query not in expanded_queries:
                                expanded_queries.append(expanded_query)
                                expansion_log.append({
                                    'type': 'single_expansion',
                                    'original_term': main_term,
                                    'synonym': synonym,
                                    'original_query': working_query,
                                    'expanded_query': expanded_query
                                })
            
            # ë‹¤ì¤‘ ìš©ì–´ ì¡°í•© í™•ì¥
            query_terms = []
            for main_term in self.synonyms.keys():
                if main_term in working_query:
                    query_terms.append(main_term)
            
            if len(query_terms) >= 2:
                for i in range(len(query_terms)):
                    for j in range(i+1, len(query_terms)):
                        term1, term2 = query_terms[i], query_terms[j]
                        
                        for synonym1 in self.synonyms[term1]:
                            if synonym1 != term1:
                                for synonym2 in self.synonyms[term2]:
                                    if synonym2 != term2:
                                        expanded_query = working_query.replace(term1, synonym1).replace(term2, synonym2)
                                        if expanded_query not in expanded_queries:
                                            expanded_queries.append(expanded_query)
                                            expansion_log.append({
                                                'type': 'double_expansion',
                                                'original_terms': [term1, term2],
                                                'synonyms': [synonym1, synonym2],
                                                'original_query': working_query,
                                                'expanded_query': expanded_query
                                            })
        
        elif self.expansion_mode == "both":
            # í†µí•© ëª¨ë“œ: ì •ê·œí™” + í™•ì¥ ëª¨ë‘ ìˆ˜í–‰
            # 1. ì •ê·œí™”
            normalized_query = self.normalize_synonyms_to_main_terms(query)
            if normalized_query != query:
                expanded_queries.append(normalized_query)
                expansion_log.append({
                    'type': 'normalization',
                    'original_query': query,
                    'normalized_query': normalized_query,
                    'description': 'ìœ ì‚¬ì–´ë¥¼ ëŒ€í‘œì–´ë¡œ ì •ê·œí™”'
                })
            
            # 2. í™•ì¥
            working_query = normalized_query if normalized_query != query else query
            
            # ë‹¨ì¼ ìš©ì–´ í™•ì¥
            for main_term, synonyms in self.synonyms.items():
                if main_term in working_query:
                    for synonym in synonyms:
                        if synonym != main_term:
                            expanded_query = working_query.replace(main_term, synonym)
                            if expanded_query not in expanded_queries:
                                expanded_queries.append(expanded_query)
                                expansion_log.append({
                                    'type': 'single_expansion',
                                    'original_term': main_term,
                                    'synonym': synonym,
                                    'original_query': working_query,
                                    'expanded_query': expanded_query
                                })
            
            # ë‹¤ì¤‘ ìš©ì–´ ì¡°í•© í™•ì¥
            query_terms = []
            for main_term in self.synonyms.keys():
                if main_term in working_query:
                    query_terms.append(main_term)
            
            if len(query_terms) >= 2:
                for i in range(len(query_terms)):
                    for j in range(i+1, len(query_terms)):
                        term1, term2 = query_terms[i], query_terms[j]
                        
                        for synonym1 in self.synonyms[term1]:
                            if synonym1 != term1:
                                for synonym2 in self.synonyms[term2]:
                                    if synonym2 != term2:
                                        expanded_query = working_query.replace(term1, synonym1).replace(term2, synonym2)
                                        if expanded_query not in expanded_queries:
                                            expanded_queries.append(expanded_query)
                                            expansion_log.append({
                                                'type': 'double_expansion',
                                                'original_terms': [term1, term2],
                                                'synonyms': [synonym1, synonym2],
                                                'original_query': working_query,
                                                'expanded_query': expanded_query
                                            })
        
        # í™•ì¥ ë¡œê·¸ ì €ì¥
        self.last_expansion_log = expansion_log
        return expanded_queries
    
    def normalize_synonyms_to_main_terms(self, query: str) -> str:
        """ìœ ì‚¬ì–´ë¥¼ ëŒ€í‘œì–´ë¡œ ì •ê·œí™”"""
        normalized_query = query
        
        # ìœ ì‚¬ì–´ â†’ ëŒ€í‘œì–´ ë§¤í•‘ ìƒì„±
        synonym_to_main = {}
        for main_term, synonyms in self.synonyms.items():
            for synonym in synonyms:
                if synonym != main_term:  # ìê¸° ìì‹ ì€ ì œì™¸
                    synonym_to_main[synonym] = main_term
        
        # ì¿¼ë¦¬ì—ì„œ ìœ ì‚¬ì–´ë¥¼ ëŒ€í‘œì–´ë¡œ êµì²´
        for synonym, main_term in synonym_to_main.items():
            if synonym in normalized_query:
                normalized_query = normalized_query.replace(synonym, main_term)
        
        return normalized_query
    
    def get_normalization_info(self, query: str) -> Dict:
        """ì •ê·œí™” ì •ë³´ ë°˜í™˜"""
        original_query = query
        normalized_query = self.normalize_synonyms_to_main_terms(query)
        
        # ì–´ë–¤ ìœ ì‚¬ì–´ê°€ ì–´ë–¤ ëŒ€í‘œì–´ë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ì¶”ì 
        changes = []
        synonym_to_main = {}
        for main_term, synonyms in self.synonyms.items():
            for synonym in synonyms:
                if synonym != main_term:
                    synonym_to_main[synonym] = main_term
        
        for synonym, main_term in synonym_to_main.items():
            if synonym in original_query:
                changes.append({
                    'synonym': synonym,
                    'main_term': main_term,
                    'position': original_query.find(synonym)
                })
        
        return {
            'original_query': original_query,
            'normalized_query': normalized_query,
            'changed': original_query != normalized_query,
            'changes': changes
        }
    
    def set_expansion_mode(self, mode: str):
        """í™•ì¥ ëª¨ë“œ ì„¤ì •"""
        valid_modes = ["normalize", "expand", "both"]
        if mode in valid_modes:
            self.expansion_mode = mode
            print(f"âœ… í™•ì¥ ëª¨ë“œê°€ '{mode}'ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë“œì…ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“œ: {valid_modes}")
    
    def get_expansion_mode(self) -> str:
        """í˜„ì¬ í™•ì¥ ëª¨ë“œ ë°˜í™˜"""
        return self.expansion_mode
    
    def get_expansion_log(self) -> List[Dict]:
        """ë§ˆì§€ë§‰ í™•ì¥ ê³¼ì • ë¡œê·¸ ë°˜í™˜"""
        return getattr(self, 'last_expansion_log', [])
    
    def add_synonym(self, main_term: str, synonym: str):
        """ìƒˆë¡œìš´ ìœ ì‚¬ì–´ ì¶”ê°€"""
        if main_term not in self.synonyms:
            self.synonyms[main_term] = []
        
        if synonym not in self.synonyms[main_term]:
            self.synonyms[main_term].append(synonym)
            self._save_synonyms()
    
    def _save_synonyms(self):
        """ìœ ì‚¬ì–´ íŒŒì¼ ì €ì¥"""
        try:
            with open(self.synonym_file, 'w', encoding='utf-8') as f:
                json.dump(self.synonyms, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ìœ ì‚¬ì–´ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def show_synonyms(self, term: str = None):
        """ìœ ì‚¬ì–´ Dictionary ìƒíƒœ í‘œì‹œ"""
        if term:
            if term in self.synonyms:
                print(f"\nğŸ“š '{term}' ê´€ë ¨ ìœ ì‚¬ì–´:")
                for i, synonym in enumerate(self.synonyms[term], 1):
                    print(f"  {i}. {synonym}")
            else:
                print(f"âŒ '{term}'ì— ëŒ€í•œ ìœ ì‚¬ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"\nğŸ“š ì „ì²´ ìœ ì‚¬ì–´ Dictionary ìƒíƒœ:")
            print(f"  ì´ ìš©ì–´ ìˆ˜: {len(self.synonyms)}")
            print(f"  ì´ ìœ ì‚¬ì–´ ìˆ˜: {sum(len(synonyms) for synonyms in self.synonyms.values())}")
            print("\n  ì£¼ìš” ìš©ì–´ë“¤:")
            for term, synonyms in list(self.synonyms.items())[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                print(f"    {term}: {len(synonyms)}ê°œ ìœ ì‚¬ì–´")
            if len(self.synonyms) > 10:
                print(f"    ... ì™¸ {len(self.synonyms) - 10}ê°œ ìš©ì–´")
    
    def test_expansion(self, query: str):
        """ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ§ª ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸:")
        print(f"ì›ë³¸ ì¿¼ë¦¬: '{query}'")
        
        # ì •ê·œí™” ì •ë³´ í‘œì‹œ
        normalization_info = self.get_normalization_info(query)
        if normalization_info['changed']:
            print(f"\nğŸ“ ì •ê·œí™” ê³¼ì •:")
            print(f"  ì›ë³¸: '{normalization_info['original_query']}'")
            print(f"  ì •ê·œí™”: '{normalization_info['normalized_query']}'")
            print(f"  ë³€ê²½ëœ ìš©ì–´ë“¤:")
            for change in normalization_info['changes']:
                print(f"    '{change['synonym']}' â†’ '{change['main_term']}'")
        
        expanded_queries = self.expand_query(query)
        expansion_log = self.get_expansion_log()
        
        print(f"\nğŸ“Š í™•ì¥ ê²°ê³¼:")
        print(f"  í™•ì¥ëœ ì¿¼ë¦¬ ìˆ˜: {len(expanded_queries)}")
        print("  í™•ì¥ëœ ì¿¼ë¦¬ë“¤:")
        for i, expanded_query in enumerate(expanded_queries, 1):
            print(f"    {i}. {expanded_query}")
        
        if expansion_log:
            print(f"\nğŸ”„ í™•ì¥ ê³¼ì •:")
            for i, log_entry in enumerate(expansion_log, 1):
                print(f"  {i}. {log_entry}")
        
        return expanded_queries

class QueryOptimizer:
    """ì¿¼ë¦¬ ìµœì í™” íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, synonym_dict: SynonymDictionary):
        self.synonym_dict = synonym_dict
        self.query_history = []
    
    def preprocess_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì „ì²˜ë¦¬"""
        # 1. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        query = re.sub(r'\s+', ' ', query.strip())
        
        # 2. íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        query = re.sub(r'[^\w\sê°€-í£]', ' ', query)
        
        # 3. ìˆ«ì ì •ê·œí™”
        query = re.sub(r'(\d+)ì²œë§Œì›', r'\1ì²œë§Œì›', query)
        query = re.sub(r'(\d+)ë§Œì›', r'\1ë§Œì›', query)
        
        return query
    
    def expand_query_with_synonyms(self, query: str) -> List[str]:
        """ìœ ì‚¬ì–´ë¥¼ ì‚¬ìš©í•œ ì¿¼ë¦¬ í™•ì¥"""
        expanded_queries = self.synonym_dict.expand_query(query)
        
        # í™•ì¥ ê³¼ì • ì‹œê°í™”
        expansion_log = self.synonym_dict.get_expansion_log()
        if expansion_log:
            print("\nğŸ”„ ìœ ì‚¬ì–´ í™•ì¥ ê³¼ì •:")
            print("=" * 60)
            for i, log_entry in enumerate(expansion_log, 1):
                print(f"  ë‹¨ê³„ {i} ({log_entry.get('type', 'unknown')}):")
                
                if log_entry.get('type') == 'normalization':
                    print(f"    ì„¤ëª…: {log_entry.get('description', 'ì •ê·œí™”')}")
                    print(f"    ë³€ê²½ ì „: '{log_entry['original_query']}'")
                    print(f"    ë³€ê²½ í›„: '{log_entry['normalized_query']}'")
                elif log_entry.get('type') in ['single_replacement', 'single_expansion']:
                    print(f"    ì›ë³¸ ìš©ì–´: '{log_entry['original_term']}'")
                    print(f"    ìœ ì‚¬ì–´: '{log_entry['synonym']}'")
                    print(f"    ë³€ê²½ ì „: '{log_entry['original_query']}'")
                    print(f"    ë³€ê²½ í›„: '{log_entry['expanded_query']}'")
                elif log_entry.get('type') in ['double_replacement', 'double_expansion']:
                    print(f"    ì›ë³¸ ìš©ì–´ë“¤: {log_entry['original_terms']}")
                    print(f"    ìœ ì‚¬ì–´ë“¤: {log_entry['synonyms']}")
                    print(f"    ë³€ê²½ ì „: '{log_entry['original_query']}'")
                    print(f"    ë³€ê²½ í›„: '{log_entry['expanded_query']}'")
                
                print("-" * 40)
            print("=" * 60)
        
        return expanded_queries
    
    def generate_search_queries(self, original_query: str) -> List[str]:
        """ê²€ìƒ‰ìš© ì¿¼ë¦¬ ìƒì„±"""
        current_mode = self.synonym_dict.get_expansion_mode()
        mode_descriptions = {
            "normalize": "ì •ê·œí™” ëª¨ë“œ (ìœ ì‚¬ì–´ â†’ ëŒ€í‘œì–´)",
            "expand": "í™•ì¥ ëª¨ë“œ (ëŒ€í‘œì–´ â†’ ìœ ì‚¬ì–´)",
            "both": "í†µí•© ëª¨ë“œ (ì •ê·œí™” + í™•ì¥)"
        }
        
        print("\nğŸ”§ ì¿¼ë¦¬ ìµœì í™” íŒŒì´í”„ë¼ì¸:")
        print(f"í˜„ì¬ ëª¨ë“œ: {current_mode} - {mode_descriptions.get(current_mode, 'ì•Œ ìˆ˜ ì—†ìŒ')}")
        print("=" * 60)
        
        # 1. ì „ì²˜ë¦¬
        processed_query = self.preprocess_query(original_query)
        print(f"1ï¸âƒ£ ì „ì²˜ë¦¬:")
        print(f"   ì›ë³¸: '{original_query}'")
        print(f"   ì²˜ë¦¬ í›„: '{processed_query}'")
        print("-" * 40)
        
        # 2. ìœ ì‚¬ì–´ ì²˜ë¦¬ (ëª¨ë“œì— ë”°ë¼ ë‹¤ë¦„)
        expanded_queries = self.expand_query_with_synonyms(processed_query)
        
        if current_mode == "normalize":
            # ì •ê·œí™” ëª¨ë“œ: ì •ê·œí™” ì •ë³´ë§Œ í‘œì‹œ
            normalization_info = self.synonym_dict.get_normalization_info(processed_query)
            if normalization_info['changed']:
                print(f"2ï¸âƒ£ ìœ ì‚¬ì–´ ì •ê·œí™”:")
                print(f"   ì›ë³¸: '{normalization_info['original_query']}'")
                print(f"   ì •ê·œí™”: '{normalization_info['normalized_query']}'")
                print(f"   ë³€ê²½ëœ ìš©ì–´ë“¤:")
                for change in normalization_info['changes']:
                    print(f"     '{change['synonym']}' â†’ '{change['main_term']}'")
                print("-" * 40)
            else:
                print(f"2ï¸âƒ£ ìœ ì‚¬ì–´ ì •ê·œí™”:")
                print(f"   ì •ê·œí™”í•  ìœ ì‚¬ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("-" * 40)
        
        elif current_mode == "expand":
            # í™•ì¥ ëª¨ë“œ: í™•ì¥ ì •ë³´ë§Œ í‘œì‹œ
            print(f"2ï¸âƒ£ ìœ ì‚¬ì–´ í™•ì¥:")
            print(f"   í™•ì¥ëœ ì¿¼ë¦¬ ìˆ˜: {len(expanded_queries)}")
            for i, query in enumerate(expanded_queries, 1):
                print(f"   {i}. {query}")
            print("-" * 40)
        
        elif current_mode == "both":
            # í†µí•© ëª¨ë“œ: ì •ê·œí™” + í™•ì¥ ëª¨ë‘ í‘œì‹œ
            normalization_info = self.synonym_dict.get_normalization_info(processed_query)
            if normalization_info['changed']:
                print(f"2ï¸âƒ£ ìœ ì‚¬ì–´ ì •ê·œí™”:")
                print(f"   ì›ë³¸: '{normalization_info['original_query']}'")
                print(f"   ì •ê·œí™”: '{normalization_info['normalized_query']}'")
                print(f"   ë³€ê²½ëœ ìš©ì–´ë“¤:")
                for change in normalization_info['changes']:
                    print(f"     '{change['synonym']}' â†’ '{change['main_term']}'")
                print("-" * 40)
            
            print(f"3ï¸âƒ£ ìœ ì‚¬ì–´ í™•ì¥:")
            print(f"   í™•ì¥ëœ ì¿¼ë¦¬ ìˆ˜: {len(expanded_queries)}")
            for i, query in enumerate(expanded_queries, 1):
                print(f"   {i}. {query}")
            print("-" * 40)
        
        # ì¿¼ë¦¬ ë³€í˜• ìƒì„± (ëª¨ë“œì— ë”°ë¼ ë‹¨ê³„ ë²ˆí˜¸ ì¡°ì •)
        variations = self._generate_query_variations(processed_query)
        step_number = 3 if current_mode in ["normalize", "expand"] else 4
        print(f"{step_number}ï¸âƒ£ ì¿¼ë¦¬ ë³€í˜• ìƒì„±:")
        print(f"   ë³€í˜• ì¿¼ë¦¬ ìˆ˜: {len(variations)}")
        for i, query in enumerate(variations, 1):
            print(f"   {i}. {query}")
        print("-" * 40)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬ (ëª¨ë“œì— ë”°ë¼ ë‹¨ê³„ ë²ˆí˜¸ ì¡°ì •)
        all_queries = [processed_query] + expanded_queries + variations
        unique_queries = list(dict.fromkeys(all_queries))  # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
        
        final_step = 4 if current_mode in ["normalize", "expand"] else 5
        print(f"{final_step}ï¸âƒ£ ìµœì¢… ì •ë¦¬:")
        print(f"   ì´ ìƒì„±ëœ ì¿¼ë¦¬: {len(all_queries)}")
        print(f"   ì¤‘ë³µ ì œê±° í›„: {len(unique_queries)}")
        print(f"   ìµœì¢… ì„ íƒ: {min(5, len(unique_queries))}ê°œ")
        print("=" * 60)
        
        return unique_queries[:5]  # ìµœëŒ€ 5ê°œ ì¿¼ë¦¬ ë°˜í™˜
    
    def _generate_query_variations(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ ë³€í˜• ìƒì„±"""
        variations = []
        
        # 1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(query)
        
        # 2. í‚¤ì›Œë“œ ì¡°í•© ìƒì„±
        if len(keywords) >= 2:
            # 2ê°œ í‚¤ì›Œë“œ ì¡°í•©
            for i in range(len(keywords)):
                for j in range(i+1, len(keywords)):
                    combination = f"{keywords[i]} {keywords[j]}"
                    if combination not in variations:
                        variations.append(combination)
        
        # 3. ì§ˆë¬¸ í˜•íƒœ ë³€í˜•
        question_patterns = [
            f"{query} ê³„ì‚° ë°©ë²•",
            f"{query} ì„¸ìœ¨",
            f"{query} ê³µì œ",
            f"{query} ì‹ ê³ "
        ]
        variations.extend(question_patterns)
        
        return variations
    
    def _extract_keywords(self, query: str) -> List[str]:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì„¸ê¸ˆ ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œ
        tax_keywords = [
            "ì†Œë“ì„¸", "ì—°ë´‰", "ê±°ì£¼ì", "ê³¼ì„¸í‘œì¤€", "ì¢…í•©ì†Œë“",
            "ì„¸ìœ¨", "ê³µì œ", "ì‹ ê³ ", "ë‚©ë¶€", "ê³„ì‚°",
            "ë²•ì¸ì„¸", "ë¶€ê°€ê°€ì¹˜ì„¸", "ì–‘ë„ì†Œë“ì„¸", "ìƒì†ì„¸", "ì¦ì—¬ì„¸"
        ]
        
        extracted = []
        for keyword in tax_keywords:
            if keyword in query:
                extracted.append(keyword)
        
        return extracted
    
    def log_query(self, original_query: str, optimized_queries: List[str]):
        """ì¿¼ë¦¬ ë¡œê¹…"""
        self.query_history.append({
            'original': original_query,
            'optimized': optimized_queries,
            'timestamp': str(datetime.now())
        })

class EnhancedRAGSystem:
    """í–¥ìƒëœ RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""
    
    def __init__(self, database, llm, synonym_dict: SynonymDictionary):
        self.database = database
        self.llm = llm
        self.synonym_dict = synonym_dict
        self.query_optimizer = QueryOptimizer(synonym_dict)
    
    def search_with_optimization(self, query: str, top_k: int = 5) -> List[Tuple]:
        """ìµœì í™”ëœ ê²€ìƒ‰ ìˆ˜í–‰"""
        # 1. ì¿¼ë¦¬ ìµœì í™”
        optimized_queries = self.query_optimizer.generate_search_queries(query)
        
        # 2. ê° ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ìˆ˜í–‰
        all_results = []
        for opt_query in optimized_queries:
            try:
                results = self.database.similarity_search_with_score(opt_query, k=top_k)
                all_results.extend(results)
            except Exception as e:
                print(f"âš ï¸  ì¿¼ë¦¬ '{opt_query}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # 3. ê²°ê³¼ ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_results = self._deduplicate_results(all_results)
        
        # 4. ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        return sorted(unique_results, key=lambda x: x[1])[:top_k]
    
    def _deduplicate_results(self, results: List[Tuple]) -> List[Tuple]:
        """ê²€ìƒ‰ ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen_content = set()
        unique_results = []
        
        for doc, score in results:
            content_hash = hash(doc.page_content[:100])  # ë‚´ìš©ì˜ ì²˜ìŒ 100ìë¡œ í•´ì‹œ
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_results.append((doc, score))
        
        return unique_results

class ConversationHistory:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, max_history: int = 10):
        self.history = []
        self.max_history = max_history
        self.current_context = ""
        self.history_enabled = True  # íˆìŠ¤í† ë¦¬ í™œì„±í™” ìƒíƒœ
    
    def add_exchange(self, question: str, answer: str, retrieved_docs: List = None):
        """ì§ˆë¬¸-ë‹µë³€ êµí™˜ ì¶”ê°€"""
        if not self.history_enabled:
            return  # íˆìŠ¤í† ë¦¬ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ì €ì¥í•˜ì§€ ì•ŠìŒ
        
        exchange = {
            'timestamp': datetime.now().isoformat(),
            'question': question,
            'answer': answer,
            'retrieved_docs': retrieved_docs if retrieved_docs else [],
            'context_summary': self._extract_context(answer)
        }
        
        self.history.append(exchange)
        
        # ìµœëŒ€ íˆìŠ¤í† ë¦¬ ìˆ˜ ìœ ì§€
        if len(self.history) > self.max_history:
            self.history.pop(0)
        
        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        self._update_context()
    
    def _extract_context(self, answer: str) -> str:
        """ë‹µë³€ì—ì„œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥)
        keywords = ['ì†Œë“ì„¸', 'ì„¸ìœ¨', 'ê³µì œ', 'ê³¼ì„¸í‘œì¤€', 'ì—°ë´‰', 'ê±°ì£¼ì']
        context_parts = []
        
        for keyword in keywords:
            if keyword in answer:
                # í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ì¥ ì¶”ì¶œ
                sentences = answer.split('.')
                for sentence in sentences:
                    if keyword in sentence:
                        context_parts.append(sentence.strip())
        
        return '. '.join(context_parts[:3])  # ìµœëŒ€ 3ê°œ ë¬¸ì¥
    
    def _update_context(self):
        """ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸"""
        if not self.history:
            self.current_context = ""
            return
        
        # ìµœê·¼ 3ê°œ êµí™˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²°í•©
        recent_contexts = []
        for exchange in self.history[-3:]:
            if exchange['context_summary']:
                recent_contexts.append(exchange['context_summary'])
        
        self.current_context = " ".join(recent_contexts)
    
    def get_context_for_query(self, new_question: str) -> str:
        """ìƒˆ ì§ˆë¬¸ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        if not self.history_enabled or not self.history:
            return new_question
        
        # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì™€ ìƒˆ ì§ˆë¬¸ ê²°í•©
        context_query = f"ì´ì „ ëŒ€í™”: {self.current_context}\n\nìƒˆ ì§ˆë¬¸: {new_question}"
        return context_query
    
    def show_history(self, limit: int = 5):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ"""
        if not self.history_enabled:
            print("ğŸš« ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        if not self.history:
            print("ğŸ“ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“ ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœëŒ€ {limit}ê°œ):")
        print("=" * 60)
        
        for i, exchange in enumerate(self.history[-limit:], 1):
            print(f"\nğŸ’¬ êµí™˜ {i}:")
            print(f"  ì§ˆë¬¸: {exchange['question']}")
            print(f"  ë‹µë³€: {exchange['answer'][:100]}...")
            print(f"  ì»¨í…ìŠ¤íŠ¸: {exchange['context_summary']}")
            print("-" * 40)
        
        print("=" * 60)
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        removed_count = len(self.history)
        self.history = []
        self.current_context = ""
        print(f"ğŸ—‘ï¸  ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (ì œê±°ëœ ëŒ€í™”: {removed_count}ê°œ)")
    
    def reset_conversation(self):
        """ëŒ€í™” ì™„ì „ ì´ˆê¸°í™” (íˆìŠ¤í† ë¦¬ + ì»¨í…ìŠ¤íŠ¸)"""
        removed_count = len(self.history)
        self.history = []
        self.current_context = ""
        self.history_enabled = True  # ê¸°ë³¸ ìƒíƒœë¡œ ë³µì›
        print(f"ğŸ”„ ëŒ€í™”ê°€ ì™„ì „íˆ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"  - ì œê±°ëœ ëŒ€í™”: {removed_count}ê°œ")
        print(f"  - íˆìŠ¤í† ë¦¬ ìƒíƒœ: í™œì„±í™”ë¡œ ë³µì›")
        print(f"  - ì»¨í…ìŠ¤íŠ¸: ì´ˆê¸°í™”ë¨")
    
    def clear_and_disable(self):
        """íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” í›„ ë¹„í™œì„±í™”"""
        removed_count = len(self.history)
        self.history = []
        self.current_context = ""
        self.history_enabled = False
        print(f"ğŸš« ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ê³  ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"  - ì œê±°ëœ ëŒ€í™”: {removed_count}ê°œ")
        print(f"  - í–¥í›„ ëŒ€í™”ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def disable_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¹„í™œì„±í™”"""
        self.history_enabled = False
        print("ğŸš« ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def enable_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ í™œì„±í™”"""
        self.history_enabled = True
        print("âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def remove_last_exchange(self):
        """ë§ˆì§€ë§‰ ëŒ€í™” êµí™˜ ì œê±°"""
        if self.history:
            removed = self.history.pop()
            self._update_context()
            print(f"ğŸ—‘ï¸  ë§ˆì§€ë§‰ ëŒ€í™”ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤:")
            print(f"  ì§ˆë¬¸: {removed['question']}")
            print(f"  ë‹µë³€: {removed['answer'][:50]}...")
        else:
            print("âŒ ì œê±°í•  ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def remove_exchange_by_index(self, index: int):
        """ì¸ë±ìŠ¤ë¡œ íŠ¹ì • ëŒ€í™” êµí™˜ ì œê±°"""
        if 0 <= index < len(self.history):
            removed = self.history.pop(index)
            self._update_context()
            print(f"ğŸ—‘ï¸  ëŒ€í™” {index + 1}ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤:")
            print(f"  ì§ˆë¬¸: {removed['question']}")
            print(f"  ë‹µë³€: {removed['answer'][:50]}...")
        else:
            print(f"âŒ ì¸ë±ìŠ¤ {index + 1}ì˜ ëŒ€í™”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    def get_history_status(self) -> Dict:
        """íˆìŠ¤í† ë¦¬ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            'enabled': self.history_enabled,
            'count': len(self.history),
            'max_history': self.max_history,
            'has_context': bool(self.current_context)
        }
    
    def get_relevant_context(self, new_question: str) -> str:
        """ìƒˆ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì°¾ê¸°"""
        if not self.history_enabled or not self.history:
            return ""
        
        # ìƒˆ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = self._extract_keywords(new_question)
        
        relevant_contexts = []
        for exchange in self.history:
            # ì´ì „ ì§ˆë¬¸ì´ë‚˜ ë‹µë³€ì—ì„œ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
            exchange_text = f"{exchange['question']} {exchange['answer']}"
            exchange_keywords = self._extract_keywords(exchange_text)
            
            # í‚¤ì›Œë“œ ê²¹ì¹¨ í™•ì¸
            common_keywords = set(question_keywords) & set(exchange_keywords)
            if common_keywords:
                relevant_contexts.append(exchange['context_summary'])
        
        return " ".join(relevant_contexts[-2:])  # ìµœê·¼ 2ê°œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = ['ì†Œë“ì„¸', 'ì„¸ìœ¨', 'ê³µì œ', 'ê³¼ì„¸í‘œì¤€', 'ì—°ë´‰', 'ê±°ì£¼ì', 'ì¢…í•©ì†Œë“', 'ë²•ì¸ì„¸', 'ë¶€ê°€ê°€ì¹˜ì„¸']
        found_keywords = []
        
        for keyword in keywords:
            if keyword in text:
                found_keywords.append(keyword)
        
        return found_keywords

def check_existing_database():
    """ê¸°ì¡´ Chroma ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    chroma_dir = Path("./chroma")
    if chroma_dir.exists() and any(chroma_dir.iterdir()):
        return True
    return False

def clear_database():
    """ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ"""
    chroma_dir = Path("./chroma")
    if chroma_dir.exists():
        shutil.rmtree(chroma_dir)
        print("ğŸ—‘ï¸  ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

def get_user_choice():
    """ì‚¬ìš©ìë¡œë¶€í„° ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ë°©í–¥ ì„ íƒë°›ê¸°"""
    print("\nğŸ“‹ ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ì˜µì…˜:")
    print("1. ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì¬ì‚¬ìš© (ë¹ ë¦„)")
    print("2. ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ì²˜ìŒ ì‹¤í–‰)")
    print("3. ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (1-3): ").strip()
            if choice in ['1', '2', '3']:
                return choice
            else:
                print("âŒ 1, 2, 3 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit()

def get_debug_mode():
    """ë””ë²„ê·¸ ëª¨ë“œ ì„ íƒ"""
    print("\nğŸ” ë””ë²„ê·¸ ì˜µì…˜:")
    print("1. ê¸°ë³¸ ëª¨ë“œ (í”„ë¡¬í”„íŠ¸ ë¯¸í‘œì‹œ)")
    print("2. í”„ë¡¬í”„íŠ¸ í™•ì¸ ëª¨ë“œ (LLM ì „ë‹¬ í”„ë¡¬í”„íŠ¸ í‘œì‹œ)")
    
    while True:
        try:
            choice = input("\nì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
            if choice in ['1', '2']:
                return choice == '2'
            else:
                print("âŒ 1, 2 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            exit()



def main():
    # LangChain ë””ë²„ê·¸ ë¡œê¹… í™œì„±í™” (ì„ íƒì‚¬í•­)
    # logging.basicConfig(level=logging.DEBUG)
    
    # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # Google API í‚¤ í™•ì¸
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— GOOGLE_API_KEY=your_api_key_hereë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
        return
    
    try:
        # ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¸
        has_existing_db = check_existing_database()
        
        if has_existing_db:
            choice = get_user_choice()
            
            if choice == '3':
                clear_database()
                has_existing_db = False
        else:
            print("ğŸ“„ ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ë””ë²„ê·¸ ëª¨ë“œ ì„ íƒ
        debug_mode = get_debug_mode()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        print("\nğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ì‚¬ìš©ìë¡œë¶€í„° ì„ë² ë”© ëª¨ë¸ ì„ íƒë°›ê¸°
        print("\nğŸ“Š ì„ë² ë”© ëª¨ë¸ ì„ íƒ:")
        print("1. í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ (ko-sroberta-multitask) - ì¶”ì²œ")
        print("2. Google Gemini ì„ë² ë”© (ê¸°ì¡´)")
        
        while True:
            try:
                embedding_choice = input("\nì„ íƒí•˜ì„¸ìš” (1-2): ").strip()
                if embedding_choice in ['1', '2']:
                    break
                else:
                    print("âŒ 1, 2 ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                exit()
        
        if embedding_choice == '1':
            # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            print("ğŸ‡°ğŸ‡· í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            model_name = "jhgan/ko-sroberta-multitask"
            model_kwargs = {'device': 'cpu'}
            encode_kwargs = {'normalize_embeddings': True}
            embedding = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs=model_kwargs,
                encode_kwargs=encode_kwargs
            )
            print("âœ… í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        else:
            # Google Gemini ì„ë² ë”© ì‚¬ìš© (ê¸°ì¡´)
            print("ğŸŒ Google Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            embedding = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
            print("âœ… Google Gemini ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ") 

        # Chroma ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        database = Chroma(
            collection_name='chroma-tax', 
            persist_directory="./chroma", 
            embedding_function=embedding
        )

        # ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬
        if has_existing_db and choice == '1':
            print("ğŸ“š ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¬ì‚¬ìš© ì¤‘...")
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì˜ ë¬¸ì„œ ìˆ˜ í™•ì¸
            try:
                collection_count = database._collection.count()
                print(f"âœ… ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {collection_count}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âš ï¸  ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                print("   ìƒˆë¡œìš´ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                has_existing_db = False
        
        if not has_existing_db or choice == '2':
            print("ğŸ“„ ë¬¸ì„œ ë¡œë”© ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ë¶„í•  ì„¤ì •
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1500,
                chunk_overlap=200,
            )

            # Word ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
            loader = Docx2txtLoader("./tax.docx")
            document_list = loader.load_and_split(text_splitter=text_splitter)
            
            print(f"âœ… {len(document_list)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

            # ë¬¸ì„œë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            database.add_documents(document_list)
            print("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")

        # LLM ëª¨ë¸ ì´ˆê¸°í™”
        llm = ChatGoogleGenerativeAI(model='gemini-2.0-flash', temperature=0.9)
        prompt = hub.pull("rlm/rag-prompt")

        # RAG ì²´ì¸ ìƒì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm, 
            retriever=database.as_retriever(),
            chain_type_kwargs={"prompt": prompt}
        )
        
        # ìœ ì‚¬ì–´ Dictionary ë° í–¥ìƒëœ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("\nğŸ“š ìœ ì‚¬ì–´ Dictionary ë¡œë”© ì¤‘...")
        synonym_dict = SynonymDictionary()
        enhanced_rag = EnhancedRAGSystem(database, llm, synonym_dict)
        print("âœ… ìœ ì‚¬ì–´ Dictionary ë¡œë”© ì™„ë£Œ")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        conversation_history = ConversationHistory()

        # ì§ˆë¬¸-ë‹µë³€ ë£¨í”„
        while True:
            # ì‚¬ìš©ìë¡œë¶€í„° ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
            print("\nğŸ¤– ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
            print("   (ì—¬ëŸ¬ ì¤„ ì…ë ¥ ê°€ëŠ¥: ê° ì¤„ ì…ë ¥ í›„ Enter, 'END'ë¡œ ì™„ë£Œ, 'CANCEL'ë¡œ ì·¨ì†Œ)")
            print("   (ì˜ˆ: ì œ 55ì¡°ì˜ ì¢…í•©ì†Œë“ ê³¼ì œí‘œì¤€ ê¸°ì¤€ìœ¼ë¡œ ê±°ì£¼ìì˜ ì—°ë´‰ì´ 5ì²œë§Œì› ì¸ ê²½ìš°")
            print("        í•´ë‹¹ ê±°ì£¼ìì˜ ì†Œë“ì„¸ëŠ” ì–¼ë§ˆì¸ê°€ìš”?)")
            print("   (ì…ë ¥ ì¤‘ 'CLEAR'ë¡œ ë‚´ìš© ì§€ìš°ê¸°, 'CANCEL'ë¡œ ì…ë ¥ ì·¨ì†Œ)")
            print("   (ì…ë ¥ ì¤‘ íˆìŠ¤í† ë¦¬ ì œì–´: disable_history, enable_history, clear_history, reset_conversation, clear_and_disable)")
            print("   (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥)")
            print("   (ìœ ì‚¬ì–´ ì¶”ê°€: add_synonym:ì£¼ìš©ì–´:ìœ ì‚¬ì–´)")
            print("   (ìœ ì‚¬ì–´ í™•ì¸: show_synonyms ë˜ëŠ” show_synonyms:ìš©ì–´)")
            print("   (í™•ì¥ í…ŒìŠ¤íŠ¸: test_expansion:í…ŒìŠ¤íŠ¸ì¿¼ë¦¬)")
            print("   (ëª¨ë“œ ì„¤ì •: set_mode:normalize/expand/both)")
            print("   (ëª¨ë“œ í™•ì¸: show_mode)")
            print("   (ëŒ€í™” íˆìŠ¤í† ë¦¬: show_history, clear_history, show_context)")
            print("   (íˆìŠ¤í† ë¦¬ ì œì–´: disable_history, enable_history, remove_last, remove_history:ë²ˆí˜¸, history_status)")
            print("   (ì™„ì „ ì´ˆê¸°í™”: reset_conversation, clear_and_disable)")
            print("-" * 50)
            
            try:
                # ì—¬ëŸ¬ ì¤„ ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
                print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì—¬ëŸ¬ ì¤„ ê°€ëŠ¥, 'END'ë¡œ ì…ë ¥ ì™„ë£Œ, 'CANCEL'ë¡œ ì·¨ì†Œ):")
                query_lines = []
                line_count = 0
                
                while True:
                    line_count += 1
                    line = input(f"[{line_count}]> ").strip()
                    
                    if line.lower() == 'end':
                        break
                    elif line.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        return
                    elif line.lower() in ['cancel', 'ì·¨ì†Œ', 'c']:
                        print("âŒ ì§ˆë¬¸ ì…ë ¥ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        break
                    elif line.lower() == 'clear':
                        query_lines = []
                        line_count = 0
                        print("ğŸ—‘ï¸  ì…ë ¥ ë‚´ìš©ì´ ì§€ì›Œì¡ŒìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.")
                        continue
                    # íˆìŠ¤í† ë¦¬ ì œì–´ ëª…ë ¹ì–´ë“¤
                    elif line.lower() == 'disable_history':
                        conversation_history.disable_history()
                        continue
                    elif line.lower() == 'enable_history':
                        conversation_history.enable_history()
                        continue
                    elif line.lower() == 'clear_history':
                        conversation_history.clear_history()
                        continue
                    elif line.lower() == 'reset_conversation':
                        conversation_history.reset_conversation()
                        continue
                    elif line.lower() == 'clear_and_disable':
                        conversation_history.clear_and_disable()
                        continue
                    elif line.lower() == 'remove_last':
                        conversation_history.remove_last_exchange()
                        continue
                    elif line.lower().startswith('remove_history:'):
                        try:
                            index = int(line.split(':', 1)[1].strip()) - 1
                            conversation_history.remove_exchange_by_index(index)
                        except (ValueError, IndexError):
                            print("âŒ í˜•ì‹: remove_history:ë²ˆí˜¸ (ì˜ˆ: remove_history:1)")
                        continue
                    elif line.lower() == 'history_status':
                        status = conversation_history.get_history_status()
                        print(f"\nğŸ“Š ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒíƒœ:")
                        print(f"  í™œì„±í™”: {'âœ…' if status['enabled'] else 'âŒ'}")
                        print(f"  ì €ì¥ëœ ëŒ€í™”: {status['count']}ê°œ")
                        print(f"  ìµœëŒ€ ì €ì¥: {status['max_history']}ê°œ")
                        print(f"  ì»¨í…ìŠ¤íŠ¸: {'ìˆìŒ' if status['has_context'] else 'ì—†ìŒ'}")
                        continue
                    elif line.lower() == 'show_history':
                        conversation_history.show_history()
                        continue
                    elif line.lower() == 'show_context':
                        if conversation_history.current_context:
                            print(f"\nğŸ“ í˜„ì¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸:")
                            print(f"{conversation_history.current_context}")
                        else:
                            print("ğŸ“ í˜„ì¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                    else:
                        query_lines.append(line)
                
                # CANCELë¡œ ì·¨ì†Œëœ ê²½ìš° ë‹¤ìŒ ë£¨í”„ë¡œ
                if not query_lines:
                    continue
                
                # ì—¬ëŸ¬ ì¤„ì„ í•˜ë‚˜ì˜ ì§ˆë¬¸ìœ¼ë¡œ ê²°í•©
                query = " ".join(query_lines).strip()
                
                if not query:
                    print("âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ì…ë ¥ëœ ì§ˆë¬¸ í™•ì¸
                print(f"\nğŸ“ ì…ë ¥ëœ ì§ˆë¬¸ ({len(query_lines)}ì¤„):")
                print("-" * 40)
                for i, line in enumerate(query_lines, 1):
                    print(f"  {i}. {line}")
                print("-" * 40)
                print(f"ê²°í•©ëœ ì§ˆë¬¸: {query}")
                print("-" * 40)
                
                # íˆìŠ¤í† ë¦¬ ìƒíƒœ í‘œì‹œ
                status = conversation_history.get_history_status()
                if status['enabled']:
                    print(f"ğŸ’¾ íˆìŠ¤í† ë¦¬ ìƒíƒœ: í™œì„±í™” (ì €ì¥ë¨: {status['count']}ê°œ)")
                else:
                    print(f"ğŸš« íˆìŠ¤í† ë¦¬ ìƒíƒœ: ë¹„í™œì„±í™” (ì €ì¥ë˜ì§€ ì•ŠìŒ)")
                print("-" * 40)
                
                # ìœ ì‚¬ì–´ ê´€ë¦¬ ëª…ë ¹ì–´ ì²˜ë¦¬
                if query.lower().startswith('add_synonym:'):
                    try:
                        parts = query.split(':', 2)
                        if len(parts) == 3:
                            main_term = parts[1].strip()
                            synonym = parts[2].strip()
                            synonym_dict.add_synonym(main_term, synonym)
                            print(f"âœ… ìœ ì‚¬ì–´ ì¶”ê°€ ì™„ë£Œ: '{main_term}' â†’ '{synonym}'")
                            continue
                        else:
                            print("âŒ í˜•ì‹: add_synonym:ì£¼ìš©ì–´:ìœ ì‚¬ì–´")
                            continue
                    except Exception as e:
                        print(f"âŒ ìœ ì‚¬ì–´ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                # ìœ ì‚¬ì–´ Dictionary ìƒíƒœ í™•ì¸
                if query.lower().startswith('show_synonyms:'):
                    term = query.split(':', 1)[1].strip() if ':' in query else None
                    synonym_dict.show_synonyms(term)
                    continue
                
                if query.lower() == 'show_synonyms':
                    synonym_dict.show_synonyms()
                    continue
                
                # ì¿¼ë¦¬ í™•ì¥ í…ŒìŠ¤íŠ¸
                if query.lower().startswith('test_expansion:'):
                    test_query = query.split(':', 1)[1].strip()
                    synonym_dict.test_expansion(test_query)
                    continue
                
                # í™•ì¥ ëª¨ë“œ ì„¤ì •
                if query.lower().startswith('set_mode:'):
                    mode = query.split(':', 1)[1].strip()
                    synonym_dict.set_expansion_mode(mode)
                    continue
                
                # í˜„ì¬ ëª¨ë“œ í™•ì¸
                if query.lower() == 'show_mode':
                    current_mode = synonym_dict.get_expansion_mode()
                    mode_descriptions = {
                        "normalize": "ì •ê·œí™” ëª¨ë“œ (ìœ ì‚¬ì–´ â†’ ëŒ€í‘œì–´)",
                        "expand": "í™•ì¥ ëª¨ë“œ (ëŒ€í‘œì–´ â†’ ìœ ì‚¬ì–´)",
                        "both": "í†µí•© ëª¨ë“œ (ì •ê·œí™” + í™•ì¥)"
                    }
                    print(f"\nğŸ“Š í˜„ì¬ í™•ì¥ ëª¨ë“œ: {current_mode}")
                    print(f"ì„¤ëª…: {mode_descriptions.get(current_mode, 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                    continue
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë ¨ ëª…ë ¹ì–´
                if query.lower() == 'show_history':
                    conversation_history.show_history()
                    continue
                
                if query.lower() == 'clear_history':
                    conversation_history.clear_history()
                    continue
                
                if query.lower() == 'reset_conversation':
                    conversation_history.reset_conversation()
                    continue
                
                if query.lower() == 'clear_and_disable':
                    conversation_history.clear_and_disable()
                    continue
                
                if query.lower() == 'show_context':
                    if conversation_history.current_context:
                        print(f"\nğŸ“ í˜„ì¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸:")
                        print(f"{conversation_history.current_context}")
                    else:
                        print("ğŸ“ í˜„ì¬ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # íˆìŠ¤í† ë¦¬ ì œì–´ ëª…ë ¹ì–´
                if query.lower() == 'disable_history':
                    conversation_history.disable_history()
                    continue
                
                if query.lower() == 'enable_history':
                    conversation_history.enable_history()
                    continue
                
                if query.lower() == 'remove_last':
                    conversation_history.remove_last_exchange()
                    continue
                
                if query.lower().startswith('remove_history:'):
                    try:
                        index = int(query.split(':', 1)[1].strip()) - 1  # 1-based to 0-based
                        conversation_history.remove_exchange_by_index(index)
                    except (ValueError, IndexError):
                        print("âŒ í˜•ì‹: remove_history:ë²ˆí˜¸ (ì˜ˆ: remove_history:1)")
                    continue
                
                if query.lower() == 'history_status':
                    status = conversation_history.get_history_status()
                    print(f"\nğŸ“Š ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒíƒœ:")
                    print(f"  í™œì„±í™”: {'âœ…' if status['enabled'] else 'âŒ'}")
                    print(f"  ì €ì¥ëœ ëŒ€í™”: {status['count']}ê°œ")
                    print(f"  ìµœëŒ€ ì €ì¥: {status['max_history']}ê°œ")
                    print(f"  ì»¨í…ìŠ¤íŠ¸: {'ìˆìŒ' if status['has_context'] else 'ì—†ìŒ'}")
                    continue
                
                # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ í™•ì¸ ë° ì ìš©
                relevant_context = conversation_history.get_relevant_context(query)
                if relevant_context:
                    print(f"\nğŸ”„ ê´€ë ¨ ì´ì „ ì»¨í…ìŠ¤íŠ¸ ë°œê²¬:")
                    print(f"  {relevant_context}")
                    print("-" * 50)
                
                print(f"\nğŸ¤– ì…ë ¥ëœ ì§ˆë¬¸: {query}")
                print("-" * 50)
                
                # í–¥ìƒëœ ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
                print("ğŸ” ì¿¼ë¦¬ ìµœì í™” ë° ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
                retrieved_docs_with_scores = enhanced_rag.search_with_optimization(query, top_k=5)
                retrieved_docs = [doc for doc, _ in retrieved_docs_with_scores]
                print(f"ğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
                
                # ì¿¼ë¦¬ ìµœì í™” ì •ë³´ í‘œì‹œ
                optimized_queries = enhanced_rag.query_optimizer.generate_search_queries(query)
                if len(optimized_queries) > 1:
                    print(f"ğŸ”„ ì¿¼ë¦¬ ìµœì í™”: {len(optimized_queries)}ê°œ ë³€í˜• ìƒì„±")
                    for i, opt_query in enumerate(optimized_queries[:3], 1):
                        print(f"   {i}. {opt_query}")
                    if len(optimized_queries) > 3:
                        print(f"   ... ì™¸ {len(optimized_queries)-3}ê°œ")
                
                # ì¿¼ë¦¬ ìµœì í™” ê³¼ì •ì„ ë¡œê·¸ì— ì €ì¥
                enhanced_rag.query_optimizer.log_query(query, optimized_queries)
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ë¶„ì„
                print("\nğŸ” ìœ ì‚¬ë„ ì ìˆ˜ ë¶„ì„:")
                print("=" * 60)
                for i, (doc, score) in enumerate(retrieved_docs_with_scores, 1):
                    print(f"\nğŸ“‹ ë¬¸ì„œ {i} (ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}):")
                    print(f"   í˜ì´ì§€: {doc.metadata.get('page', 'N/A')}")
                    print(f"   ì†ŒìŠ¤: {doc.metadata.get('source', 'N/A')}")
                    print(f"   ë‚´ìš© ê¸¸ì´: {len(doc.page_content)}ì")
                    print(f"   ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:200]}...")
                    print("-" * 40)
                print("=" * 60)
                
                # ìœ ì‚¬ë„ ì ìˆ˜ í•´ì„
                print("\nğŸ’¡ ìœ ì‚¬ë„ ì ìˆ˜ í•´ì„:")
                print("   - ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ë” ìœ ì‚¬í•¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)")
                print("   - ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ëœ ìœ ì‚¬í•¨")
                
                # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
                best_match = min(retrieved_docs_with_scores, key=lambda x: x[1])
                print(f"\nğŸ† ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ: ë¬¸ì„œ {retrieved_docs_with_scores.index(best_match) + 1} (ì ìˆ˜: {best_match[1]:.4f})")
                
                # ì „ì²´ ë¬¸ì„œ ë‚´ìš© ì¶œë ¥
                print("\nğŸ“„ ì „ì²´ ë¬¸ì„œ ë‚´ìš©:")
                print("=" * 60)
                for i, (doc, score) in enumerate(retrieved_docs_with_scores, 1):
                    print(f"\nğŸ“‹ ë¬¸ì„œ {i} (ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}):")
                    print(f"   ë‚´ìš©: {doc.page_content}")
                    print("-" * 40)
                print("=" * 60)

                # ë””ë²„ê·¸ ëª¨ë“œì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ í™•ì¸
                if debug_mode:
                    print("\nğŸ” LLMì— ì „ë‹¬ë˜ëŠ” í”„ë¡¬í”„íŠ¸ í™•ì¸:")
                    print("=" * 60)
                    
                    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ contextë¡œ ê²°í•©
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° í‘œì‹œ
                    print("ğŸ“‹ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°:")
                    print("Context: [ê²€ìƒ‰ëœ ë¬¸ì„œë“¤]")
                    print("Question: [ì‚¬ìš©ì ì§ˆë¬¸]")
                    print("Answer: [AI ì‘ë‹µ]")
                    print("-" * 40)
                    
                    # ì‹¤ì œ ì „ë‹¬ë˜ëŠ” ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                    print("ğŸ“ ì‹¤ì œ ì „ë‹¬ë˜ëŠ” ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
                    print(f"ì§ˆë¬¸: {query}")
                    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
                    print(f"Context ê¸¸ì´: {len(context)}ì")
                    print(f"Context ë¯¸ë¦¬ë³´ê¸°: {context[:300]}...")
                    print("=" * 60)
                    
                    # í”„ë¡¬í”„íŠ¸ ì •ë³´
                    print(f"ğŸ“Š í”„ë¡¬í”„íŠ¸ ì •ë³´:")
                    print(f"   - ì§ˆë¬¸ ê¸¸ì´: {len(query)}ì")
                    print(f"   - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}ê°œ")
                    print(f"   - Context ê¸¸ì´: {len(context)}ì")
                    print(f"   - ì´ ì „ë‹¬ ë°ì´í„°: {len(query) + len(context)}ì")
                    
                    # ìƒì„¸ ë°ì´í„° ë¶„ì„
                    print(f"\nğŸ“ˆ ìƒì„¸ ë°ì´í„° ë¶„ì„:")
                    print(f"   - ê° ë¬¸ì„œë³„ ê¸¸ì´:")
                    for i, doc in enumerate(retrieved_docs, 1):
                        doc_length = len(doc.page_content)
                        print(f"     ë¬¸ì„œ {i}: {doc_length}ì")
                    
                    # í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì  ê³„ì‚°)
                    estimated_tokens = (len(query) + len(context)) // 4  # ëŒ€ëµ 1í† í° = 4ì
                    print(f"   - ì¶”ì • í† í° ìˆ˜: ì•½ {estimated_tokens:,} í† í°")
                    
                    # ë°ì´í„° íš¨ìœ¨ì„± ë¶„ì„
                    avg_doc_length = len(context) / len(retrieved_docs) if retrieved_docs else 0
                    print(f"   - í‰ê·  ë¬¸ì„œ ê¸¸ì´: {avg_doc_length:.0f}ì")
                    print(f"   - ë°ì´í„° íš¨ìœ¨ì„±: {len(query)}ì ì§ˆë¬¸ â†’ {len(context)}ì ì»¨í…ìŠ¤íŠ¸")
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (UTF-8 ê¸°ì¤€)
                    memory_usage = (len(query) + len(context)) * 4  # UTF-8ì€ ìµœëŒ€ 4ë°”ì´íŠ¸
                    print(f"   - ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ {memory_usage:,} ë°”ì´íŠ¸ ({memory_usage/1024:.1f} KB)")
                    
                    # API ë¹„ìš© ì¶”ì • (Gemini ê¸°ì¤€)
                    input_tokens = estimated_tokens
                    output_tokens_estimate = 100  # ì˜ˆìƒ ì¶œë ¥ í† í° ìˆ˜
                    # Gemini 2.0 Flash ìš”ê¸ˆ (ì˜ˆì‹œ - ì‹¤ì œ ìš”ê¸ˆì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                    input_cost = (input_tokens / 1000) * 0.000075  # $0.000075 per 1K input tokens
                    output_cost = (output_tokens_estimate / 1000) * 0.0003   # $0.0003 per 1K output tokens
                    total_cost = input_cost + output_cost
                    print(f"   - ì¶”ì • API ë¹„ìš©: ì•½ ${total_cost:.6f} (ì…ë ¥: ${input_cost:.6f}, ì¶œë ¥: ${output_cost:.6f})")
                    
                    # ë°ì´í„° ì „ì†¡ ì‹œê°í™”
                    print(f"\nğŸ“Š ë°ì´í„° ì „ì†¡ ì‹œê°í™”:")
                    total_chars = len(query) + len(context)
                    query_percent = (len(query) / total_chars) * 100
                    context_percent = (len(context) / total_chars) * 100
                    
                    print(f"   ì§ˆë¬¸: {'â–ˆ' * int(query_percent/2)}{' ' * (50 - int(query_percent/2))} {query_percent:.1f}%")
                    print(f"   ì»¨í…ìŠ¤íŠ¸: {'â–ˆ' * int(context_percent/2)}{' ' * (50 - int(context_percent/2))} {context_percent:.1f}%")
                    print(f"   {'â”€' * 50}")
                    print(f"   ì´ {total_chars:,}ì ({estimated_tokens:,} í† í°)")
                    
                    # ì„±ëŠ¥ ìµœì í™” ì œì•ˆ
                    print(f"\nğŸ’¡ ì„±ëŠ¥ ìµœì í™” ì œì•ˆ:")
                    if len(context) > 8000:  # 8K í† í° ì´ìƒ
                        print(f"   - âš ï¸  ì»¨í…ìŠ¤íŠ¸ê°€ í½ë‹ˆë‹¤. ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ ì²­í¬ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
                    if len(retrieved_docs) > 5:
                        print(f"   - ğŸ’¡ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë§ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")
                    if avg_doc_length > 1000:
                        print(f"   - ğŸ’¡ ë¬¸ì„œê°€ ê¸¸ì–´ì„œ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    print("-" * 40)
                
                # ì‹¤ì œ ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
                print("ğŸ§  AI ì‘ë‹µ ìƒì„± ì¤‘...")
                
                # ìµœì¢… ì§ˆì˜ ë¡œê·¸ ì¶œë ¥
                print("\nğŸ“ ìµœì¢… ì§ˆì˜ ë¡œê·¸:")
                print("=" * 60)
                print(f"ì›ë³¸ ì§ˆë¬¸: {query}")
                print(f"ìµœì í™”ëœ ì¿¼ë¦¬ ìˆ˜: {len(optimized_queries)}")
                print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
                print(f"ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {sum(len(doc.page_content) for doc in retrieved_docs)}ì")
                print(f"í‰ê·  ë¬¸ì„œ ê¸¸ì´: {sum(len(doc.page_content) for doc in retrieved_docs) // len(retrieved_docs) if retrieved_docs else 0}ì")
                print("-" * 60)
                
                # ìµœì í™”ëœ ì¿¼ë¦¬ë“¤ í‘œì‹œ
                print("ğŸ”„ ìµœì í™”ëœ ì¿¼ë¦¬ë“¤:")
                for i, opt_query in enumerate(optimized_queries, 1):
                    print(f"  {i}. {opt_query}")
                print("-" * 60)
                
                # ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½
                print("ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½:")
                for i, (doc, score) in enumerate(retrieved_docs_with_scores, 1):
                    print(f"  ë¬¸ì„œ {i} (ìœ ì‚¬ë„: {score:.4f}): {doc.page_content[:100]}...")
                print("=" * 60)
                
                # ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì§ˆë¬¸ ìƒì„±
                context_query = conversation_history.get_context_for_query(query)
                
                # RAG ì²´ì¸ì— ì „ë‹¬ë˜ëŠ” ì •ë³´
                print("ğŸ§  RAG ì²´ì¸ ì…ë ¥ ì •ë³´:")
                print(f"  - ì›ë³¸ ì§ˆë¬¸: {query}")
                if conversation_history.current_context:
                    print(f"  - ì´ì „ ì»¨í…ìŠ¤íŠ¸: {conversation_history.current_context[:100]}...")
                print(f"  - ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
                print(f"  - ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {sum(len(doc.page_content) for doc in retrieved_docs)}ì")
                print(f"  - ì‚¬ìš© ëª¨ë¸: Gemini 2.0 Flash")
                print(f"  - Temperature: 0.9")
                print("=" * 60)
                
                # RAG ì²´ì¸ ì‹¤í–‰ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
                ai_message = qa_chain({"query": context_query})

                print("\n" + "=" * 50)
                print("âœ… ìµœì¢… ì‘ë‹µ:")
                print(ai_message['result'])
                print("=" * 50)
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— êµí™˜ ì¶”ê°€
                conversation_history.add_exchange(query, ai_message['result'], retrieved_docs_with_scores)

            except KeyboardInterrupt:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            except Exception as e:
                print(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("   ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                continue
        
    except FileNotFoundError:
        print("âŒ tax.docx íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— tax.docx íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()